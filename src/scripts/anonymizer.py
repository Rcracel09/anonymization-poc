"""
Enhanced anonymization logic with improved PII detection
Melhorias:
1. Detec√ß√£o mais precisa com scoring system
2. Melhor tratamento de edge cases
3. Performance otimizada
4. Valida√ß√£o de dados mais robusta
5. Suporte a mais padr√µes de PII
"""

import spacy
import re
from faker import Faker
from typing import Dict, Optional, List, Tuple, Set
from collections import defaultdict
import unicodedata

class Anonymizer:
    def __init__(self, locale: str = 'pt_PT', confidence_threshold: float = 0.6):
        """
        Inicializa o anonimizador com modelo spaCy portugu√™s
        
        Args:
            locale: Locale para Faker
            confidence_threshold: Threshold de confian√ßa para detec√ß√£o (0.0 a 1.0)
        """
        print("üì¶ Carregando modelo spaCy portugu√™s...")
        self.nlp = spacy.load("pt_core_news_lg")
        self.fake = Faker(locale)
        self.confidence_threshold = confidence_threshold
        
        # Dicion√°rios para consist√™ncia
        self.name_mapping: Dict[str, str] = {}
        self.email_mapping: Dict[str, str] = {}
        
        # Estat√≠sticas detalhadas
        self.stats = defaultdict(int)
        
        # Padr√µes regex otimizados
        self.email_pattern = re.compile(
            r'\b[A-Za-z0-9](?:[A-Za-z0-9._%+-]*[A-Za-z0-9])?@[A-Za-z0-9](?:[A-Za-z0-9.-]*[A-Za-z0-9])?\.[A-Z|a-z]{2,}\b',
            re.IGNORECASE
        )
        
        # Padr√£o para nomes (2-5 palavras capitalizadas)
        self.name_pattern = re.compile(
            r'\b[A-Z√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á][a-z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß]{1,}(?:\s+(?:de|da|do|dos|das|e|van|von|del|della|di|O\'|Mc|Mac))?\s+[A-Z√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á][a-z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß]{1,}(?:\s+[A-Z√Å√Ä√Ç√É√Ñ√â√à√ä√ã√ç√å√é√è√ì√í√î√ï√ñ√ö√ô√õ√ú√á][a-z√°√†√¢√£√§√©√®√™√´√≠√¨√Æ√Ø√≥√≤√¥√µ√∂√∫√π√ª√º√ß]{1,}){0,3}\b'
        )
        
        # Keywords expandidas e ponderadas
        self.email_keywords = {
            # Peso alto (1.0)
            'email': 1.0, 'e-mail': 1.0, 'mail': 0.9, 'correio': 1.0, 'correo': 0.9,
            # Peso m√©dio (0.7)
            'contact_email': 1.0, 'work_email': 1.0, 'personal_email': 1.0,
            'electronic_mail': 0.8,
            # Peso baixo (0.5)
            'contact': 0.5, 'contato': 0.5
        }
        
        self.name_keywords = {
            # Peso alto (1.0)
            'name': 1.0, 'nome': 1.0, 'full_name': 1.0, 'fullname': 1.0,
            'first_name': 1.0, 'last_name': 1.0, 'firstname': 1.0, 'lastname': 1.0,
            # Peso m√©dio-alto (0.8-0.9)
            'author': 0.9, 'autor': 0.9, 'creator': 0.8, 'criador': 0.8,
            'reviewer': 0.9, 'revisor': 0.9, 'approver': 0.8, 'aprovador': 0.8,
            'owner': 0.7, 'proprietario': 0.7, 'responsible': 0.7, 'responsavel': 0.7,
            # Peso m√©dio (0.6-0.7)
            'person': 0.7, 'pessoa': 0.7, 'contact': 0.6, 'contato': 0.6,
            'client': 0.7, 'cliente': 0.7, 'customer': 0.7,
            # Peso baixo (0.5)
            'user': 0.5, 'usuario': 0.5, 'assigned': 0.5,
            'member': 0.5, 'membro': 0.5, 'participant': 0.5
        }
        
        # Keywords de exclus√£o (certamente N√ÉO s√£o PII)
        self.exclusion_keywords = {
            'title', 'titulo', 'subject', 'assunto', 'product', 'produto',
            'item', 'project', 'projeto', 'description', 'descricao',
            'content', 'conteudo', 'text', 'texto', 'note', 'nota',
            'observation', 'observacao', 'review', 'comment', 'comentario',
            'message', 'mensagem', 'body', 'status', 'type', 'tipo',
            'category', 'categoria', 'tag', 'label', 'etiqueta'
        }
        
        # Palavras comuns (n√£o s√£o nomes)
        self.common_words = self._load_common_words()
        
        # Cache para otimiza√ß√£o
        self._spacy_cache: Dict[str, bool] = {}
    
    def _load_common_words(self) -> Set[str]:
        """
        Carrega lista de palavras comuns que n√£o s√£o nomes
        """
        return {
            # Artigos e conjun√ß√µes
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'by', 'with',
            'o', 'a', 'os', 'as', 'um', 'uma', 'e', 'ou', 'mas', 'em', 'de', 'para',
            # Tratamentos (podem aparecer mas n√£o s√£o nomes completos)
            'mr', 'mrs', 'ms', 'dr', 'prof', 'sr', 'sra', 'dra',
            # Contexto comum
            'contact', 'email', 'phone', 'address', 'dear', 'hello', 'regards',
            'contato', 'telefone', 'endereco', 'caro', 'ola', 'atenciosamente',
            # Dias e meses
            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
            'segunda', 'terca', 'quarta', 'quinta', 'sexta', 'sabado', 'domingo',
            'january', 'february', 'march', 'april', 'may', 'june', 'july',
            'august', 'september', 'october', 'november', 'december',
            'janeiro', 'fevereiro', 'marco', 'abril', 'maio', 'junho', 'julho',
            'agosto', 'setembro', 'outubro', 'novembro', 'dezembro',
            # Empresas e lugares comuns
            'company', 'corporation', 'limited', 'inc', 'ltd', 'group',
            'empresa', 'sociedade', 'limitada', 'grupo',
            'portugal', 'lisboa', 'porto', 'coimbra', 'brazil', 'brasil',
            'user', 'customer', 'client', 'utilizador', 'cliente'
        }
    
    def _normalize_text(self, text: str) -> str:
        """
        Normaliza texto removendo acentos para compara√ß√£o
        """
        return ''.join(
            c for c in unicodedata.normalize('NFD', text)
            if unicodedata.category(c) != 'Mn'
        ).lower()
    
    def _calculate_keyword_score(self, column_name: str, keywords: Dict[str, float]) -> float:
        """
        Calcula score baseado em keywords ponderadas
        """
        column_lower = self._normalize_text(column_name)
        max_score = 0.0
        
        for keyword, weight in keywords.items():
            if keyword in column_lower:
                # Score mais alto se keyword est√° isolada
                if column_lower == keyword:
                    max_score = max(max_score, weight * 1.2)
                # Score m√©dio se keyword est√° no in√≠cio ou fim
                elif column_lower.startswith(keyword) or column_lower.endswith(keyword):
                    max_score = max(max_score, weight * 1.1)
                # Score normal se keyword est√° contida
                else:
                    max_score = max(max_score, weight)
        
        return min(max_score, 1.0)  # Cap em 1.0
    
    def _is_excluded_column(self, column_name: str) -> bool:
        """
        Verifica se coluna deve ser exclu√≠da da detec√ß√£o
        """
        column_lower = self._normalize_text(column_name)
        return any(excl in column_lower for excl in self.exclusion_keywords)
    
    def is_email_column(self, column_name: str, sample_values: List[str]) -> Tuple[bool, float]:
        """
        Detecta se uma coluna cont√©m emails com score de confian√ßa
        
        Returns:
            (is_email, confidence_score)
        """
        # Verificar exclus√£o
        if self._is_excluded_column(column_name):
            return False, 0.0
        
        # Score baseado em keywords
        keyword_score = self._calculate_keyword_score(column_name, self.email_keywords)
        
        # Se n√£o h√° samples, usar apenas keyword
        if not sample_values or len(sample_values) == 0:
            return keyword_score >= self.confidence_threshold, keyword_score
        
        # Validar samples
        valid_samples = [v for v in sample_values if v and isinstance(v, str) and len(str(v).strip()) > 0]
        
        if not valid_samples:
            return keyword_score >= self.confidence_threshold, keyword_score
        
        # Score baseado em conte√∫do
        email_count = 0
        for val in valid_samples:
            val_str = str(val).strip()
            # Email v√°lido deve ter @ e dom√≠nio
            if '@' in val_str and self.email_pattern.fullmatch(val_str):
                email_count += 1
        
        content_score = email_count / len(valid_samples) if valid_samples else 0.0
        
        # Score final: m√©dia ponderada (keyword 40%, content 60%)
        final_score = (keyword_score * 0.4) + (content_score * 0.6)
        
        self.stats['email_detection_attempts'] += 1
        if final_score >= self.confidence_threshold:
            self.stats['email_detection_success'] += 1
        
        return final_score >= self.confidence_threshold, final_score
    
    def is_name_column(self, column_name: str, sample_values: List[str]) -> Tuple[bool, float]:
        """
        Detecta se uma coluna cont√©m nomes com score de confian√ßa
        
        Returns:
            (is_name, confidence_score)
        """
        # Verificar exclus√£o
        if self._is_excluded_column(column_name):
            return False, 0.0
        
        # Score baseado em keywords
        keyword_score = self._calculate_keyword_score(column_name, self.name_keywords)
        
        # Se n√£o h√° samples, usar apenas keyword
        if not sample_values or len(sample_values) == 0:
            return keyword_score >= self.confidence_threshold, keyword_score
        
        # Validar samples
        valid_samples = [
            v for v in sample_values 
            if v and isinstance(v, str) and 3 <= len(str(v).strip()) <= 150
        ]
        
        if not valid_samples:
            return keyword_score >= self.confidence_threshold, keyword_score
        
        # An√°lise de conte√∫do
        name_indicators = 0
        sample_limit = min(len(valid_samples), 20)  # Limitar an√°lise
        
        for val in valid_samples[:sample_limit]:
            val_str = str(val).strip()
            
            # Skip valores muito curtos ou muito longos
            if len(val_str) < 3 or len(val_str) > 150:
                continue
            
            # M√©todo 1: Heur√≠stica r√°pida
            if self._looks_like_name(val_str):
                name_indicators += 1
                continue
            
            # M√©todo 2: spaCy (mais lento, usar com cache)
            if self._is_person_entity(val_str):
                name_indicators += 1
        
        content_score = name_indicators / sample_limit if sample_limit > 0 else 0.0
        
        # Score final: m√©dia ponderada (keyword 35%, content 65%)
        final_score = (keyword_score * 0.35) + (content_score * 0.65)
        
        self.stats['name_detection_attempts'] += 1
        if final_score >= self.confidence_threshold:
            self.stats['name_detection_success'] += 1
        
        return final_score >= self.confidence_threshold, final_score
    
    def _looks_like_name(self, text: str) -> bool:
        """
        Heur√≠stica otimizada para verificar se texto parece um nome
        """
        if not text or len(text) < 3 or len(text) > 150:
            return False
        
        # Normalizar espa√ßos
        text = ' '.join(text.split())
        words = text.split()
        
        # Deve ter 2-5 palavras
        if len(words) < 2 or len(words) > 5:
            return False
        
        # Contar palavras capitalizadas (excluindo conectores)
        connectors = {'de', 'da', 'do', 'dos', 'das', 'e', 'van', 'von', 'del', 'della', 'di'}
        capitalized = sum(
            1 for w in words 
            if w and w[0].isupper() and w.lower() not in connectors
        )
        
        # Verificar se n√£o cont√©m palavras comuns
        normalized_words = [self._normalize_text(w) for w in words]
        if any(w in self.common_words for w in normalized_words):
            return False
        
        # Pelo menos 60% das palavras (exceto conectores) devem estar capitalizadas
        non_connector_words = [w for w in words if w.lower() not in connectors]
        if not non_connector_words:
            return False
        
        cap_ratio = capitalized / len(non_connector_words)
        
        # Verificar se n√£o cont√©m n√∫meros
        has_numbers = any(char.isdigit() for char in text)
        
        # Verificar se n√£o cont√©m caracteres especiais (exceto acentos e h√≠fen)
        has_special = bool(re.search(r'[^a-zA-Z√Ä-√ø\s\'-]', text))
        
        return cap_ratio >= 0.6 and not has_numbers and not has_special
    
    def _is_person_entity(self, text: str) -> bool:
        """
        Usa spaCy para verificar se √© uma entidade PERSON (com cache)
        """
        # Verificar cache
        if text in self._spacy_cache:
            return self._spacy_cache[text]
        
        try:
            doc = self.nlp(text)
            is_person = any(ent.label_ == "PER" for ent in doc.ents)
            
            # Cachear resultado
            self._spacy_cache[text] = is_person
            
            # Limitar tamanho do cache
            if len(self._spacy_cache) > 1000:
                # Remover 20% mais antigos
                items_to_remove = list(self._spacy_cache.keys())[:200]
                for key in items_to_remove:
                    del self._spacy_cache[key]
            
            return is_person
        except Exception:
            return False
    
    def detect_pii_columns(self, column_samples: Dict[str, List[str]]) -> Dict[str, Tuple[str, float]]:
        """
        Detecta automaticamente colunas com PII
        
        Returns:
            {column_name: (pii_type, confidence_score)}
        """
        pii_columns = {}
        
        print("\nüîç Detectando colunas com PII...")
        
        for column_name, sample_values in column_samples.items():
            # Filtrar valores None/NULL
            sample_values = [v for v in sample_values if v is not None]
            
            if not sample_values:
                continue
            
            # Testar email primeiro (mais espec√≠fico)
            is_email, email_score = self.is_email_column(column_name, sample_values)
            if is_email:
                pii_columns[column_name] = ('email', email_score)
                print(f"   ‚úì {column_name} ‚Üí EMAIL (confian√ßa: {email_score:.2%})")
                continue
            
            # Testar nome
            is_name, name_score = self.is_name_column(column_name, sample_values)
            if is_name:
                pii_columns[column_name] = ('name', name_score)
                print(f"   ‚úì {column_name} ‚Üí NAME (confian√ßa: {name_score:.2%})")
                continue
        
        return pii_columns
    
    def anonymize_name(self, original_name: str) -> str:
        """
        Anonimiza um nome com valida√ß√£o aprimorada
        """
        if not original_name:
            return original_name
        
        name_str = str(original_name).strip()
        
        if not name_str or len(name_str) < 2:
            return original_name
        
        # Normalizar espa√ßos
        name_str = ' '.join(name_str.split())
        
        if name_str not in self.name_mapping:
            # Gerar nome fake
            self.name_mapping[name_str] = self.fake.name()
            self.stats['unique_names_anonymized'] += 1
        
        self.stats['total_name_operations'] += 1
        return self.name_mapping[name_str]
    
    def anonymize_email(self, original_email: str) -> str:
        """
        Anonimiza email com valida√ß√£o e normaliza√ß√£o aprimorada
        """
        if not original_email or '@' not in str(original_email):
            return original_email
        
        email_str = str(original_email).strip().lower()
        
        # Validar formato b√°sico
        if not self.email_pattern.fullmatch(email_str):
            return original_email
        
        if email_str not in self.email_mapping:
            # Gerar email v√°lido
            fake_email = self.fake.email()
            
            # Garantir formato v√°lido (sem acentos, espa√ßos)
            fake_email = self._sanitize_email(fake_email)
            
            self.email_mapping[email_str] = fake_email
            self.stats['unique_emails_anonymized'] += 1
        
        self.stats['total_email_operations'] += 1
        return self.email_mapping[email_str]
    
    def _sanitize_email(self, email: str) -> str:
        """
        Remove caracteres inv√°lidos de email
        """
        if '@' not in email:
            return email
        
        local_part, domain = email.split('@', 1)
        
        # Substituir acentos
        local_part = self._normalize_text(local_part)
        
        # Remover caracteres inv√°lidos
        local_part = re.sub(r'[^a-z0-9._+-]', '', local_part)
        
        # Garantir que n√£o come√ßa/termina com ponto
        local_part = local_part.strip('.')
        
        return f"{local_part}@{domain}"
    
    def anonymize_text(self, text: str) -> str:
        """
        Anonimiza PII em texto livre com melhor performance
        """
        if not text or not isinstance(text, str):
            return text
        
        text_str = str(text).strip()
        
        if len(text_str) < 10:  # Muito curto para conter PII relevante
            return text
        
        anonymized_text = text_str
        replacements = []  # Lista de (start, end, original, replacement)
        
        # Fase 1: Detectar e coletar emails
        for email_match in self.email_pattern.finditer(text_str):
            original_email = email_match.group()
            anonymized_email = self.anonymize_email(original_email)
            
            if original_email != anonymized_email:
                replacements.append((
                    email_match.start(),
                    email_match.end(),
                    original_email,
                    anonymized_email
                ))
        
        # Fase 2: Detectar nomes com spaCy
        try:
            doc = self.nlp(text_str)
            
            for ent in doc.ents:
                if ent.label_ != "PER":
                    continue
                
                # Skip se overlap com email
                overlaps = any(
                    start <= ent.start_char < end or start < ent.end_char <= end
                    for start, end, _, _ in replacements
                )
                if overlaps:
                    continue
                
                original_name = ent.text
                
                # Validar se realmente parece nome
                if not self._looks_like_name(original_name):
                    continue
                
                anonymized_name = self.anonymize_name(original_name)
                
                if original_name != anonymized_name:
                    replacements.append((
                        ent.start_char,
                        ent.end_char,
                        original_name,
                        anonymized_name
                    ))
        except Exception as e:
            self.stats['spacy_errors'] += 1
            # Continuar com replacements j√° coletados
        
        # Fase 3: Aplicar substitui√ß√µes de tr√°s para frente
        replacements.sort(key=lambda x: x[0], reverse=True)
        
        for start, end, original, replacement in replacements:
            anonymized_text = (
                anonymized_text[:start] +
                replacement +
                anonymized_text[end:]
            )
            self.stats['text_replacements'] += 1
        
        return anonymized_text
    
    def get_statistics(self) -> Dict:
        """
        Retorna estat√≠sticas detalhadas da anonimiza√ß√£o
        """
        return {
            'detection': {
                'email_attempts': self.stats.get('email_detection_attempts', 0),
                'email_success': self.stats.get('email_detection_success', 0),
                'name_attempts': self.stats.get('name_detection_attempts', 0),
                'name_success': self.stats.get('name_detection_success', 0),
            },
            'anonymization': {
                'unique_names': self.stats.get('unique_names_anonymized', 0),
                'unique_emails': self.stats.get('unique_emails_anonymized', 0),
                'total_name_ops': self.stats.get('total_name_operations', 0),
                'total_email_ops': self.stats.get('total_email_operations', 0),
                'text_replacements': self.stats.get('text_replacements', 0),
            },
            'errors': {
                'spacy_errors': self.stats.get('spacy_errors', 0),
            },
            'mappings': {
                'total_names_mapped': len(self.name_mapping),
                'total_emails_mapped': len(self.email_mapping),
            },
            'sample_mappings': {
                'names': dict(list(self.name_mapping.items())[:5]),
                'emails': dict(list(self.email_mapping.items())[:3])
            },
            'cache': {
                'spacy_cache_size': len(self._spacy_cache)
            }
        }
    
    def clear_cache(self):
        """
        Limpa caches para liberar mem√≥ria
        """
        self._spacy_cache.clear()
        self.stats['cache_clears'] = self.stats.get('cache_clears', 0) + 1